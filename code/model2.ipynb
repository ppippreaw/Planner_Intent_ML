{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import scipy.sparse as sp\n",
    "import joblib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        text = ''.join(char.lower() for char in text if char not in string.punctuation)\n",
    "        filtered_words = [word for word in text.split() if word not in self.stop_words]\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "        return lemmatized_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, features, labels=None):\n",
    "        self.features = torch.FloatTensor(features.toarray() if sp.issparse(features) else features)\n",
    "        self.labels = torch.LongTensor(labels) if labels is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.features[idx], self.labels[idx]\n",
    "        return self.features[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentClassifierNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(IntentClassifierNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.layer3 = nn.Linear(hidden_size // 2, num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.batch_norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.batch_norm2 = nn.LayerNorm(hidden_size // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.batch_norm1(self.layer1(x))))\n",
    "        x = self.dropout(self.relu(self.batch_norm2(self.layer2(x))))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepIntentClassifier:\n",
    "    def __init__(self, max_features=1000, hidden_size=256, batch_size=32, num_epochs=10, learning_rate=0.001, accumulation_steps=1):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.bert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.max_features = max_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.accumulation_steps = accumulation_steps \n",
    "        self.model = None\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def bert_encode(self, texts):\n",
    "        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "    def extract_keywords(self, query, top_n=5):\n",
    "        tfidf_matrix = self.tfidf_vectorizer.transform([query])\n",
    "        feature_array = np.array(self.tfidf_vectorizer.get_feature_names_out())\n",
    "        tfidf_sorting = np.argsort(tfidf_matrix.toarray()).flatten()[::-1]\n",
    "        return feature_array[tfidf_sorting][:top_n].tolist()\n",
    "\n",
    "    def prepare_data(self, df):\n",
    "        try:\n",
    "            if 'query' not in df.columns:\n",
    "                raise ValueError(\"DataFrame must contain a 'query' column\")\n",
    "            if 'intent_category' not in df.columns:\n",
    "                raise ValueError(\"DataFrame must contain an 'intent_category' column\")\n",
    "            \n",
    "            processed_queries = [' '.join(self.preprocessor.preprocess(q)) for q in df['query']]\n",
    "            class_counts = df['intent_category'].value_counts()\n",
    "            min_samples = class_counts.min()\n",
    "            \n",
    "            print(f\"Class distribution:\\n{class_counts}\")\n",
    "            print(f\"Minimum samples per class: {min_samples}\")\n",
    "            \n",
    "            if min_samples >= 2:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    processed_queries,\n",
    "                    df['intent_category'],\n",
    "                    test_size=0.2,\n",
    "                    random_state=42,\n",
    "                    stratify=df['intent_category']\n",
    "                )\n",
    "                print(\"Using stratified split\")\n",
    "            else:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    processed_queries,\n",
    "                    df['intent_category'],\n",
    "                    test_size=0.2,\n",
    "                    random_state=42\n",
    "                )\n",
    "                print(\"Using regular split (not stratified)\")\n",
    "            \n",
    "            self.tfidf_vectorizer.fit(X_train)\n",
    "            return X_train, X_test, y_train, y_test\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in data preparation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_features(self, X):\n",
    "        X_tfidf = self.tfidf_vectorizer.transform(X)\n",
    "        X_bert = self.bert_encode(X)\n",
    "        return sp.hstack((X_tfidf, X_bert))\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        try:\n",
    "            if len(X_train) < self.batch_size:\n",
    "                self.batch_size = max(2, len(X_train))  # Ensure batch size is at least 2\n",
    "                print(f\"Batch size adjusted to {self.batch_size} due to small dataset\")\n",
    "            \n",
    "            X_train_combined = self.create_features(X_train)\n",
    "            y_train_encoded = self.label_encoder.fit_transform(y_train)\n",
    "            \n",
    "            input_size = X_train_combined.shape[1]\n",
    "            num_classes = len(self.label_encoder.classes_)\n",
    "            self.model = IntentClassifierNN(input_size, self.hidden_size, num_classes).to(self.device)\n",
    "            \n",
    "            train_dataset = IntentDataset(X_train_combined, y_train_encoded)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "            \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "            \n",
    "            for epoch in range(self.num_epochs):\n",
    "                self.model.train()\n",
    "                total_loss = 0\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}')\n",
    "                \n",
    "                for i, (batch_features, batch_labels) in enumerate(progress_bar):\n",
    "                    batch_features = batch_features.to(self.device)\n",
    "                    batch_labels = batch_labels.to(self.device)\n",
    "                    \n",
    "                    outputs = self.model(batch_features)\n",
    "                    loss = criterion(outputs, batch_labels)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    \n",
    "                    if (i + 1) % self.accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    progress_bar.set_postfix({'loss': total_loss / len(train_loader)})\n",
    "            \n",
    "            self.is_fitted = True\n",
    "            return self\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during model fitting: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def predict(self, queries):\n",
    "        self.model.eval()\n",
    "        processed_queries = [' '.join(self.preprocessor.preprocess(q)) for q in queries]\n",
    "        features = self.create_features(processed_queries)\n",
    "        dataset = IntentDataset(features)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size)\n",
    "        \n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch_features in dataloader:\n",
    "                batch_features = batch_features.to(self.device)\n",
    "                outputs = self.model(batch_features)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                predictions.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        return self.label_encoder.inverse_transform(predictions)\n",
    "    \n",
    "    def predict_with_keywords(self, query):\n",
    "        prediction = self.predict([query])[0]\n",
    "        keywords = self.extract_keywords(query)\n",
    "        return prediction, keywords\n",
    "\n",
    "    def predict_with_confidence(self, queries):\n",
    "        self.model.eval()\n",
    "        processed_queries = [' '.join(self.preprocessor.preprocess(q)) for q in queries]\n",
    "        features = self.create_features(processed_queries)\n",
    "        dataset = IntentDataset(features)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size)\n",
    "        \n",
    "        all_probabilities = []\n",
    "        with torch.no_grad():\n",
    "            for batch_features in dataloader:\n",
    "                batch_features = batch_features.to(self.device)\n",
    "                outputs = self.model(batch_features)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                all_probabilities.extend(probabilities.cpu().numpy())\n",
    "        \n",
    "        predictions = np.argmax(all_probabilities, axis=1)\n",
    "        confidences = np.max(all_probabilities, axis=1)\n",
    "        predicted_intents = self.label_encoder.inverse_transform(predictions)\n",
    "        \n",
    "        return list(zip(predicted_intents, confidences))\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        return classification_report(y_test, y_pred, target_names=self.label_encoder.classes_)\n",
    "\n",
    "    # def save_model(self, filepath):\n",
    "    #     model_state = {\n",
    "    #         'model_state_dict': self.model.state_dict(),\n",
    "    #         'tfidf_vectorizer': self.tfidf_vectorizer,\n",
    "    #         'label_encoder': self.label_encoder,\n",
    "    #     }\n",
    "    #     torch.save(model_state, filepath)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        model_state = torch.load(filepath)\n",
    "        self.tfidf_vectorizer = model_state['tfidf_vectorizer']\n",
    "        self.label_encoder = model_state['label_encoder']\n",
    "        \n",
    "        input_size = self.tfidf_vectorizer.max_features + 768  # BERT features size\n",
    "        num_classes = len(self.label_encoder.classes_)\n",
    "        self.model = IntentClassifierNN(input_size, self.hidden_size, num_classes).to(self.device)\n",
    "        self.model.load_state_dict(model_state['model_state_dict'])\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path = '../data/user_intention.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(json_string):\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    queries = []\n",
    "    possible_answers = []\n",
    "    intents = []\n",
    "    intent_categories = []\n",
    "    \n",
    "    for item in data:\n",
    "        intent = item['intent']\n",
    "        intent_category = item['intent_category']\n",
    "        \n",
    "        for query_item in item['user_queries']:\n",
    "            queries.append(query_item['query'])\n",
    "            possible_answers.append(query_item['possible_answers'])\n",
    "            intents.append(intent)\n",
    "            intent_categories.append(intent_category)\n",
    "\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'query': queries,\n",
    "        'possible_answers': possible_answers,\n",
    "        'intent': intents,\n",
    "        'intent_category': intent_categories\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        \n",
    "        df = load_json_data(path)\n",
    "        \n",
    "        classifier = DeepIntentClassifier(\n",
    "            max_features=1000,\n",
    "            hidden_size=256,\n",
    "            batch_size=2,  \n",
    "            num_epochs=100   \n",
    "        )\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train, X_test, y_train, y_test = classifier.prepare_data(df)\n",
    "        \n",
    "        # Train model\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        evaluation_report = classifier.evaluate(X_test, y_test)\n",
    "        print(\"Model Evaluation:\")\n",
    "        print(evaluation_report)\n",
    "        \n",
    "     \n",
    "        example_queries = [\"What is the best hotel in Japan?\"]\n",
    "        predictions = classifier.predict(example_queries)\n",
    "        \n",
    "        for query, intent in zip(example_queries, predictions):\n",
    "            keywords = classifier.predict_with_keywords(query)\n",
    "            print(f\"Query: '{query}'\")\n",
    "            print(f\"Predicted Intent: '{intent}'\")\n",
    "            print(f\"Keywords: {keywords}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "intent_category\n",
      "SearchCity              13\n",
      "SearchFood              12\n",
      "SearchTransportation     7\n",
      "SearchActivity           7\n",
      "PlanCity                 7\n",
      "PlanFood                 7\n",
      "PlanTransportation       7\n",
      "PlanActivity             7\n",
      "SearchAttractions        6\n",
      "PlanTravel               6\n",
      "PlanAccommodation        5\n",
      "PlanBudget               5\n",
      "CulturalExperience       5\n",
      "GeneralTravel            5\n",
      "Name: count, dtype: int64\n",
      "Minimum samples per class: 5\n",
      "Using stratified split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 40/40 [00:00<00:00, 273.01it/s, loss=2.76]\n",
      "Epoch 2/100: 100%|██████████| 40/40 [00:00<00:00, 282.88it/s, loss=2.59]\n",
      "Epoch 3/100: 100%|██████████| 40/40 [00:00<00:00, 283.75it/s, loss=2.27]\n",
      "Epoch 4/100: 100%|██████████| 40/40 [00:00<00:00, 270.36it/s, loss=2.06]\n",
      "Epoch 5/100: 100%|██████████| 40/40 [00:00<00:00, 279.72it/s, loss=1.78]\n",
      "Epoch 6/100: 100%|██████████| 40/40 [00:00<00:00, 273.89it/s, loss=1.59]\n",
      "Epoch 7/100: 100%|██████████| 40/40 [00:00<00:00, 280.64it/s, loss=1.42] \n",
      "Epoch 8/100: 100%|██████████| 40/40 [00:00<00:00, 284.77it/s, loss=1.15] \n",
      "Epoch 9/100: 100%|██████████| 40/40 [00:00<00:00, 276.12it/s, loss=0.987]\n",
      "Epoch 10/100: 100%|██████████| 40/40 [00:00<00:00, 275.62it/s, loss=0.802]\n",
      "Epoch 11/100: 100%|██████████| 40/40 [00:00<00:00, 109.03it/s, loss=0.786]\n",
      "Epoch 12/100: 100%|██████████| 40/40 [00:00<00:00, 145.71it/s, loss=0.58] \n",
      "Epoch 13/100: 100%|██████████| 40/40 [00:00<00:00, 219.44it/s, loss=0.618]\n",
      "Epoch 14/100: 100%|██████████| 40/40 [00:00<00:00, 229.44it/s, loss=0.503]\n",
      "Epoch 15/100: 100%|██████████| 40/40 [00:00<00:00, 224.15it/s, loss=0.48] \n",
      "Epoch 16/100: 100%|██████████| 40/40 [00:00<00:00, 227.16it/s, loss=0.43] \n",
      "Epoch 17/100: 100%|██████████| 40/40 [00:00<00:00, 239.15it/s, loss=0.319]\n",
      "Epoch 18/100: 100%|██████████| 40/40 [00:00<00:00, 232.72it/s, loss=0.297]\n",
      "Epoch 19/100: 100%|██████████| 40/40 [00:00<00:00, 229.87it/s, loss=0.257]\n",
      "Epoch 20/100: 100%|██████████| 40/40 [00:00<00:00, 163.14it/s, loss=0.243]\n",
      "Epoch 21/100: 100%|██████████| 40/40 [00:00<00:00, 204.36it/s, loss=0.202]\n",
      "Epoch 22/100: 100%|██████████| 40/40 [00:00<00:00, 205.14it/s, loss=0.17]  \n",
      "Epoch 23/100: 100%|██████████| 40/40 [00:00<00:00, 192.71it/s, loss=0.122] \n",
      "Epoch 24/100: 100%|██████████| 40/40 [00:00<00:00, 209.09it/s, loss=0.141] \n",
      "Epoch 25/100: 100%|██████████| 40/40 [00:00<00:00, 199.82it/s, loss=0.132] \n",
      "Epoch 26/100: 100%|██████████| 40/40 [00:00<00:00, 183.68it/s, loss=0.121] \n",
      "Epoch 27/100: 100%|██████████| 40/40 [00:00<00:00, 206.60it/s, loss=0.203] \n",
      "Epoch 28/100: 100%|██████████| 40/40 [00:00<00:00, 204.00it/s, loss=0.148] \n",
      "Epoch 29/100: 100%|██████████| 40/40 [00:00<00:00, 225.71it/s, loss=0.153] \n",
      "Epoch 30/100: 100%|██████████| 40/40 [00:00<00:00, 148.03it/s, loss=0.172] \n",
      "Epoch 31/100: 100%|██████████| 40/40 [00:00<00:00, 163.91it/s, loss=0.0999]\n",
      "Epoch 32/100: 100%|██████████| 40/40 [00:00<00:00, 219.80it/s, loss=0.0726]\n",
      "Epoch 33/100: 100%|██████████| 40/40 [00:00<00:00, 206.56it/s, loss=0.0962]\n",
      "Epoch 34/100: 100%|██████████| 40/40 [00:00<00:00, 218.98it/s, loss=0.116] \n",
      "Epoch 35/100: 100%|██████████| 40/40 [00:00<00:00, 214.28it/s, loss=0.0803]\n",
      "Epoch 36/100: 100%|██████████| 40/40 [00:00<00:00, 209.32it/s, loss=0.0728]\n",
      "Epoch 37/100: 100%|██████████| 40/40 [00:00<00:00, 230.95it/s, loss=0.0722]\n",
      "Epoch 38/100: 100%|██████████| 40/40 [00:00<00:00, 237.04it/s, loss=0.0595]\n",
      "Epoch 39/100: 100%|██████████| 40/40 [00:00<00:00, 169.43it/s, loss=0.0844]\n",
      "Epoch 40/100: 100%|██████████| 40/40 [00:00<00:00, 222.57it/s, loss=0.0817]\n",
      "Epoch 41/100: 100%|██████████| 40/40 [00:00<00:00, 228.24it/s, loss=0.0735]\n",
      "Epoch 42/100: 100%|██████████| 40/40 [00:00<00:00, 224.46it/s, loss=0.0758]\n",
      "Epoch 43/100: 100%|██████████| 40/40 [00:00<00:00, 233.64it/s, loss=0.0816]\n",
      "Epoch 44/100: 100%|██████████| 40/40 [00:00<00:00, 225.67it/s, loss=0.0571] \n",
      "Epoch 45/100: 100%|██████████| 40/40 [00:00<00:00, 208.03it/s, loss=0.0315] \n",
      "Epoch 46/100: 100%|██████████| 40/40 [00:00<00:00, 232.18it/s, loss=0.0534]\n",
      "Epoch 47/100: 100%|██████████| 40/40 [00:00<00:00, 209.80it/s, loss=0.0492]\n",
      "Epoch 48/100: 100%|██████████| 40/40 [00:00<00:00, 166.18it/s, loss=0.0387]\n",
      "Epoch 49/100: 100%|██████████| 40/40 [00:00<00:00, 229.36it/s, loss=0.0357]\n",
      "Epoch 50/100: 100%|██████████| 40/40 [00:00<00:00, 227.99it/s, loss=0.0504]\n",
      "Epoch 51/100: 100%|██████████| 40/40 [00:00<00:00, 231.79it/s, loss=0.0861]\n",
      "Epoch 52/100: 100%|██████████| 40/40 [00:00<00:00, 223.08it/s, loss=0.105] \n",
      "Epoch 53/100: 100%|██████████| 40/40 [00:00<00:00, 206.87it/s, loss=0.0526]\n",
      "Epoch 54/100: 100%|██████████| 40/40 [00:00<00:00, 226.37it/s, loss=0.0574]\n",
      "Epoch 55/100: 100%|██████████| 40/40 [00:00<00:00, 234.66it/s, loss=0.216] \n",
      "Epoch 56/100: 100%|██████████| 40/40 [00:00<00:00, 159.62it/s, loss=0.736]\n",
      "Epoch 57/100: 100%|██████████| 40/40 [00:00<00:00, 231.00it/s, loss=0.857]\n",
      "Epoch 58/100: 100%|██████████| 40/40 [00:00<00:00, 210.75it/s, loss=0.445]\n",
      "Epoch 59/100: 100%|██████████| 40/40 [00:00<00:00, 224.96it/s, loss=0.285]\n",
      "Epoch 60/100: 100%|██████████| 40/40 [00:00<00:00, 231.66it/s, loss=0.312]\n",
      "Epoch 61/100: 100%|██████████| 40/40 [00:00<00:00, 229.61it/s, loss=0.111] \n",
      "Epoch 62/100: 100%|██████████| 40/40 [00:00<00:00, 225.85it/s, loss=0.173]\n",
      "Epoch 63/100: 100%|██████████| 40/40 [00:00<00:00, 169.45it/s, loss=0.2]   \n",
      "Epoch 64/100: 100%|██████████| 40/40 [00:00<00:00, 207.58it/s, loss=0.0851]\n",
      "Epoch 65/100: 100%|██████████| 40/40 [00:00<00:00, 233.01it/s, loss=0.0637]\n",
      "Epoch 66/100: 100%|██████████| 40/40 [00:00<00:00, 228.28it/s, loss=0.0395]\n",
      "Epoch 67/100: 100%|██████████| 40/40 [00:00<00:00, 225.06it/s, loss=0.0515]\n",
      "Epoch 68/100: 100%|██████████| 40/40 [00:00<00:00, 230.57it/s, loss=0.0339] \n",
      "Epoch 69/100: 100%|██████████| 40/40 [00:00<00:00, 228.85it/s, loss=0.0409]\n",
      "Epoch 70/100: 100%|██████████| 40/40 [00:00<00:00, 214.07it/s, loss=0.0247]\n",
      "Epoch 71/100: 100%|██████████| 40/40 [00:00<00:00, 167.18it/s, loss=0.0213]\n",
      "Epoch 72/100: 100%|██████████| 40/40 [00:00<00:00, 226.04it/s, loss=0.0447]\n",
      "Epoch 73/100: 100%|██████████| 40/40 [00:00<00:00, 232.74it/s, loss=0.0546]\n",
      "Epoch 74/100: 100%|██████████| 40/40 [00:00<00:00, 228.50it/s, loss=0.0313]\n",
      "Epoch 75/100: 100%|██████████| 40/40 [00:00<00:00, 233.21it/s, loss=0.0454]\n",
      "Epoch 76/100: 100%|██████████| 40/40 [00:00<00:00, 208.07it/s, loss=0.0363]\n",
      "Epoch 77/100: 100%|██████████| 40/40 [00:00<00:00, 224.61it/s, loss=0.0413]\n",
      "Epoch 78/100: 100%|██████████| 40/40 [00:00<00:00, 161.93it/s, loss=0.0278]\n",
      "Epoch 79/100: 100%|██████████| 40/40 [00:00<00:00, 196.32it/s, loss=0.0377]\n",
      "Epoch 80/100: 100%|██████████| 40/40 [00:00<00:00, 229.78it/s, loss=0.0223] \n",
      "Epoch 81/100: 100%|██████████| 40/40 [00:00<00:00, 229.47it/s, loss=0.0354]\n",
      "Epoch 82/100: 100%|██████████| 40/40 [00:00<00:00, 207.49it/s, loss=0.0251] \n",
      "Epoch 83/100: 100%|██████████| 40/40 [00:00<00:00, 221.77it/s, loss=0.0326] \n",
      "Epoch 84/100: 100%|██████████| 40/40 [00:00<00:00, 162.50it/s, loss=0.0273]\n",
      "Epoch 85/100: 100%|██████████| 40/40 [00:00<00:00, 227.96it/s, loss=0.0493]\n",
      "Epoch 86/100: 100%|██████████| 40/40 [00:00<00:00, 218.73it/s, loss=0.0443]\n",
      "Epoch 87/100: 100%|██████████| 40/40 [00:00<00:00, 229.76it/s, loss=0.0307]\n",
      "Epoch 88/100: 100%|██████████| 40/40 [00:00<00:00, 210.81it/s, loss=0.0265] \n",
      "Epoch 89/100: 100%|██████████| 40/40 [00:00<00:00, 229.47it/s, loss=0.0507]\n",
      "Epoch 90/100: 100%|██████████| 40/40 [00:00<00:00, 168.90it/s, loss=0.0301]\n",
      "Epoch 91/100: 100%|██████████| 40/40 [00:00<00:00, 219.07it/s, loss=0.0279] \n",
      "Epoch 92/100: 100%|██████████| 40/40 [00:00<00:00, 225.46it/s, loss=0.0377]\n",
      "Epoch 93/100: 100%|██████████| 40/40 [00:00<00:00, 200.86it/s, loss=0.0315]\n",
      "Epoch 94/100: 100%|██████████| 40/40 [00:00<00:00, 228.44it/s, loss=0.0564]\n",
      "Epoch 95/100: 100%|██████████| 40/40 [00:00<00:00, 231.27it/s, loss=0.0246]\n",
      "Epoch 96/100: 100%|██████████| 40/40 [00:00<00:00, 164.02it/s, loss=0.0254]\n",
      "Epoch 97/100: 100%|██████████| 40/40 [00:00<00:00, 232.66it/s, loss=0.0471]\n",
      "Epoch 98/100: 100%|██████████| 40/40 [00:00<00:00, 229.11it/s, loss=0.338]\n",
      "Epoch 99/100: 100%|██████████| 40/40 [00:00<00:00, 212.73it/s, loss=0.127] \n",
      "Epoch 100/100: 100%|██████████| 40/40 [00:00<00:00, 232.29it/s, loss=0.227] \n",
      "/Users/pippreaw/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/pippreaw/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/pippreaw/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "  CulturalExperience       0.50      1.00      0.67         1\n",
      "       GeneralTravel       0.00      0.00      0.00         1\n",
      "   PlanAccommodation       0.00      0.00      0.00         1\n",
      "        PlanActivity       0.00      0.00      0.00         1\n",
      "          PlanBudget       0.00      0.00      0.00         1\n",
      "            PlanCity       0.00      0.00      0.00         1\n",
      "            PlanFood       1.00      1.00      1.00         1\n",
      "  PlanTransportation       0.50      1.00      0.67         1\n",
      "          PlanTravel       0.00      0.00      0.00         1\n",
      "      SearchActivity       0.00      0.00      0.00         2\n",
      "   SearchAttractions       0.25      1.00      0.40         1\n",
      "          SearchCity       1.00      0.67      0.80         3\n",
      "          SearchFood       1.00      1.00      1.00         3\n",
      "SearchTransportation       0.00      0.00      0.00         2\n",
      "\n",
      "            accuracy                           0.45        20\n",
      "           macro avg       0.30      0.40      0.32        20\n",
      "        weighted avg       0.41      0.45      0.41        20\n",
      "\n",
      "Query: 'What is the best hotel in Japan?'\n",
      "Predicted Intent: 'PlanAccommodation'\n",
      "Keywords: ('PlanAccommodation', ['hotel', 'what', 'best', 'japan', 'festival'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
